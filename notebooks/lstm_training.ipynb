{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichailLepin/Fake-News-Classifier/blob/main/notebooks/lstm_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9xdEg85dbXx"
      },
      "source": [
        "# LSTM Model Training for Fake News Classification\n",
        "\n",
        "This notebook contains code for training an LSTM model for fake news classification in Google Colab.\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "1. **Install Dependencies** - Install required libraries\n",
        "2. **Imports** - Import all necessary modules\n",
        "3. **Load and Process Data** - Load ISOT/Kaggle dataset and preprocessing\n",
        "4. **Build Vocabulary and Tokenization** - Build vocabulary and convert text to sequences\n",
        "5. **Load GloVe Embeddings** - Load pre-trained word embeddings\n",
        "6. **PyTorch Dataset** - Create datasets for training\n",
        "7. **LSTM Model** - Define bidirectional LSTM architecture\n",
        "8. **Training Functions** - Functions for training and evaluating the model\n",
        "9. **Training** - Model training process\n",
        "10. **Test Set Evaluation** - Final model evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryi3ojBzdbXy"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "Install all required libraries for PyTorch, data processing, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0nTUHe1dbXy",
        "outputId": "b4db7343-566c-4443-e258-e20a3a1ab670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn tqdm\n",
        "!pip install kagglehub\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im4j-1XBdbXz"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Import all necessary libraries and modules for data processing, models, and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akqnbgANdbXz",
        "outputId": "8e7a016a-f0f5-456d-b260-48e847cef859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Проверка GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN4U04ModbXz"
      },
      "source": [
        "## Load and Process Data\n",
        "\n",
        "Load ISOT/Kaggle dataset from Kaggle and perform preprocessing:\n",
        "- Load Fake.csv and True.csv via kagglehub\n",
        "- Text cleaning (lowercase, URL removal, whitespace normalization)\n",
        "- Create binary labels\n",
        "- Stratified train/validation/test split (64%/16%/20%)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utPvVQ5sdbXz",
        "outputId": "e6c1a38a-37b8-4b53-aeb9-adfc420a783e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/clmentbisaillon/fake-and-real-news-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41.0M/41.0M [00:00<00:00, 94.0MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Fake news loaded: (23481, 4)\n",
            "✓ True news loaded: (21417, 4)\n",
            "\n",
            "Using column: 'title'\n",
            "\n",
            "Cleaning text...\n",
            "\n",
            "Combined dataset: (44889, 7)\n",
            "Label distribution: {'fake': 23472, 'real': 21417}\n",
            "\n",
            "Data split:\n",
            "  Train: 28,728 (64.0%)\n",
            "  Validation: 7,183 (16.0%)\n",
            "  Test: 8,978 (20.0%)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset via kagglehub (no API keys required)\n",
        "path = kagglehub.dataset_download(\"clmentbisaillon/fake-and-real-news-dataset\")\n",
        "\n",
        "# Load data\n",
        "fake_df = pd.read_csv(f\"{path}/Fake.csv\")\n",
        "true_df = pd.read_csv(f\"{path}/True.csv\")\n",
        "\n",
        "print(f\"✓ Fake news loaded: {fake_df.shape}\")\n",
        "print(f\"✓ True news loaded: {true_df.shape}\")\n",
        "\n",
        "# Text cleaning function (from analyze_and_integrate.py script)\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text: lowercase, remove URLs, normalize whitespace\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Identify text column\n",
        "text_col = None\n",
        "for col in fake_df.columns:\n",
        "    if fake_df[col].dtype == 'object' and col.lower() in ['text', 'title', 'article']:\n",
        "        text_col = col\n",
        "        break\n",
        "if text_col is None:\n",
        "    text_col = fake_df.select_dtypes(include=['object']).columns[0]\n",
        "\n",
        "print(f\"\\nUsing column: '{text_col}'\")\n",
        "\n",
        "# Add labels\n",
        "fake_df['label'] = 'fake'\n",
        "true_df['label'] = 'real'\n",
        "\n",
        "# Combine data\n",
        "combined_data = pd.concat([fake_df, true_df], ignore_index=True)\n",
        "\n",
        "# Clean text\n",
        "print(\"\\nCleaning text...\")\n",
        "combined_data['text_cleaned'] = combined_data[text_col].apply(clean_text)\n",
        "\n",
        "# Create binary labels\n",
        "combined_data['label_binary'] = combined_data['label'].map({'fake': 1, 'real': 0})\n",
        "\n",
        "# Remove empty texts\n",
        "combined_data = combined_data[\n",
        "    combined_data['text_cleaned'].notna() &\n",
        "    (combined_data['text_cleaned'].str.len() > 0)\n",
        "]\n",
        "\n",
        "print(f\"\\nCombined dataset: {combined_data.shape}\")\n",
        "print(f\"Label distribution: {combined_data['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Split into train/val/test with stratification\n",
        "X = combined_data['text_cleaned'].values\n",
        "y = combined_data['label_binary'].values\n",
        "\n",
        "# First split: train+val (80%) and test (20%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Second split: train (64%) and val (16%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(f\"\\nData split:\")\n",
        "print(f\"  Train: {len(X_train):,} ({len(X_train)/len(combined_data)*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(X_val):,} ({len(X_val)/len(combined_data)*100:.1f}%)\")\n",
        "print(f\"  Test: {len(X_test):,} ({len(X_test)/len(combined_data)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Flmn1PdbX0"
      },
      "source": [
        "## Build Vocabulary and Tokenization\n",
        "\n",
        "Build vocabulary from training data and functions to convert text to index sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTJYuJbudbX0",
        "outputId": "12aa3b18-c84d-46b8-8a5a-fb956989c99e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Building vocabulary...\n",
            "Vocabulary size: 18321\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(texts, min_freq=2):\n",
        "    \"\"\"Build vocabulary from texts\"\"\"\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        words = str(text).lower().split()\n",
        "        word_counts.update(words)\n",
        "\n",
        "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "    idx = 2\n",
        "\n",
        "    for word, count in word_counts.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def text_to_sequence(text, vocab, max_len=256):\n",
        "    \"\"\"Convert text to sequence of indices\"\"\"\n",
        "    words = str(text).lower().split()\n",
        "    sequence = [vocab.get(word, vocab['<UNK>']) for word in words[:max_len]]\n",
        "\n",
        "    if len(sequence) < max_len:\n",
        "        sequence.extend([vocab['<PAD>']] * (max_len - len(sequence)))\n",
        "\n",
        "    return sequence[:max_len]\n",
        "\n",
        "# Build vocabulary\n",
        "print(\"\\nBuilding vocabulary...\")\n",
        "vocab = build_vocab(X_train, min_freq=2)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Model parameters\n",
        "MAX_LEN = 256\n",
        "EMBEDDING_DIM = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8eqmo5-dbX0"
      },
      "source": [
        "## Load GloVe Embeddings\n",
        "\n",
        "Load pre-trained GloVe embeddings (GloVe 6B.100d) to initialize the model's embedding layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOkG5LCydbX0",
        "outputId": "b7c7fcfc-093e-430c-e31d-0b08c99fbc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GloVe embeddings from glove.6B.100d.txt...\n",
            "Downloading GloVe 6B.100d...\n"
          ]
        }
      ],
      "source": [
        "def load_glove_embeddings(glove_path, vocab, embedding_dim=100):\n",
        "    \"\"\"Load pre-trained GloVe embeddings\"\"\"\n",
        "    print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
        "\n",
        "    if not os.path.exists(glove_path):\n",
        "        print(\"Downloading GloVe 6B.100d...\")\n",
        "        !wget -q http://nlp.stanford.edu/data/glove.6B.zip\n",
        "        !unzip -q glove.6B.zip\n",
        "\n",
        "    embeddings_index = {}\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Loading GloVe\"):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    found = 0\n",
        "\n",
        "    for word, idx in vocab.items():\n",
        "        if word in embeddings_index:\n",
        "            embedding_matrix[idx] = embeddings_index[word]\n",
        "            found += 1\n",
        "        else:\n",
        "            # Random initialization for unknown words\n",
        "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "    print(f\"Found embeddings for {found}/{vocab_size} words ({found/vocab_size*100:.2f}%)\")\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load GloVe embeddings\n",
        "GLOVE_PATH = 'glove.6B.100d.txt'\n",
        "try:\n",
        "    embedding_matrix = load_glove_embeddings(GLOVE_PATH, vocab, EMBEDDING_DIM)\n",
        "    use_pretrained = True\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Failed to load GloVe: {e}\")\n",
        "    print(\"Using random initialization\")\n",
        "    embedding_matrix = None\n",
        "    use_pretrained = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3j9XZ8zdbX0"
      },
      "source": [
        "## PyTorch Dataset\n",
        "\n",
        "Create Dataset and DataLoader classes for efficient data loading during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH13bNacdbX0"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        sequence = text_to_sequence(text, self.vocab, self.max_len)\n",
        "        return torch.LongTensor(sequence), torch.LongTensor([label])\n",
        "\n",
        "train_dataset = NewsDataset(X_train, y_train, vocab, MAX_LEN)\n",
        "val_dataset = NewsDataset(X_val, y_val, vocab, MAX_LEN)\n",
        "test_dataset = NewsDataset(X_test, y_test, vocab, MAX_LEN)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yTCShbqdbX0"
      },
      "source": [
        "## LSTM Model\n",
        "\n",
        "Define bidirectional LSTM model architecture for processing text sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z41_4ULYdbX1"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, num_layers=1,\n",
        "                 dropout=0.3, num_classes=2, embedding_matrix=None):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        if embedding_matrix is not None:\n",
        "            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "            self.embedding.weight.requires_grad = True\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        output = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "lstm_model = LSTMModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=128,\n",
        "    num_layers=1,\n",
        "    dropout=0.3,\n",
        "    num_classes=2,\n",
        "    embedding_matrix=embedding_matrix if use_pretrained else None\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\nLSTM Model Parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCjIIARfdbX1"
      },
      "source": [
        "## Training Functions\n",
        "\n",
        "Functions for training the model for one epoch and evaluating the model on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gXRPVkldbX1"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sequences, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.squeeze().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sequences)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(train_loader), 100 * correct / total\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.squeeze().to(device)\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCRuyybWdbX1"
      },
      "source": [
        "## Training\n",
        "\n",
        "LSTM model training process with early stopping based on F1-score on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyFZZ3ZFdbX1"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING LSTM MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=2e-5)\n",
        "\n",
        "num_epochs = 10\n",
        "best_f1 = 0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "val_f1s = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    train_loss, train_acc = train_epoch(lstm_model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, val_f1, _, _ = evaluate(lstm_model, val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        patience_counter = 0\n",
        "        torch.save(lstm_model.state_dict(), 'best_lstm_model.pth')\n",
        "        print(f\"✓ New best F1: {best_f1:.4f}, model saved\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Best validation F1: {best_f1:.4f}\")\n",
        "print(\"=\" * 60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXVzGGT9dbX1"
      },
      "source": [
        "## Test Set Evaluation\n",
        "\n",
        "Final model evaluation on the test set with metrics (accuracy, F1-score, precision, recall) and confusion matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKWDADxfdbX1"
      },
      "outputs": [],
      "source": [
        "lstm_model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
        "\n",
        "print(\"\\nEvaluating LSTM model on test set:\")\n",
        "test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n",
        "    lstm_model, test_loader, criterion, device\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, test_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "plt.title('LSTM - Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "lstm_results = {\n",
        "    'test_loss': float(test_loss),\n",
        "    'test_accuracy': float(test_acc),\n",
        "    'test_f1': float(test_f1),\n",
        "    'test_precision': float(precision_score(test_labels, test_preds, average='weighted')),\n",
        "    'test_recall': float(recall_score(test_labels, test_preds, average='weighted'))\n",
        "}\n",
        "\n",
        "print(f\"\\nLSTM Results: {lstm_results}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
