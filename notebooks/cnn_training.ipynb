{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Training for Fake News Classification\n",
    "\n",
    "Этот ноутбук содержит код для обучения CNN модели классификации фейковых новостей в Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Установка зависимостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers scikit-learn pandas numpy matplotlib seaborn tqdm\n",
    "!pip install gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Импорты\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Проверка GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Загрузка и обработка данных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Установка Kaggle API\n",
    "!pip install kaggle\n",
    "\n",
    "# Загрузка датасета с Kaggle\n",
    "# ВАЖНО: Сначала загрузите ваш kaggle.json файл в Colab:\n",
    "# 1. Скачайте kaggle.json с https://www.kaggle.com/settings (Account -> API -> Create New Token)\n",
    "# 2. В Colab: Files -> Upload -> выберите kaggle.json\n",
    "# 3. Или используйте: from google.colab import files; files.upload()\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Настройка Kaggle API (если kaggle.json загружен)\n",
    "if os.path.exists('/content/kaggle.json'):\n",
    "    os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
    "    !chmod 600 /content/kaggle.json\n",
    "\n",
    "# Загрузка датасета Fake and Real News\n",
    "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset -p /content --unzip\n",
    "\n",
    "# Загрузка данных\n",
    "fake_df = pd.read_csv('/content/Fake.csv')\n",
    "true_df = pd.read_csv('/content/True.csv')\n",
    "\n",
    "print(f\"✓ Fake news loaded: {fake_df.shape}\")\n",
    "print(f\"✓ True news loaded: {true_df.shape}\")\n",
    "\n",
    "# Функция очистки текста (из скрипта analyze_and_integrate.py)\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text: lowercase, remove URLs, normalize whitespace\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Идентификация текстовой колонки\n",
    "text_col = None\n",
    "for col in fake_df.columns:\n",
    "    if fake_df[col].dtype == 'object' and col.lower() in ['text', 'title', 'article']:\n",
    "        text_col = col\n",
    "        break\n",
    "if text_col is None:\n",
    "    text_col = fake_df.select_dtypes(include=['object']).columns[0]\n",
    "\n",
    "print(f\"\\nИспользуется колонка: '{text_col}'\")\n",
    "\n",
    "# Добавление меток\n",
    "fake_df['label'] = 'fake'\n",
    "true_df['label'] = 'real'\n",
    "\n",
    "# Объединение данных\n",
    "combined_data = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "\n",
    "# Очистка текста\n",
    "print(\"\\nОчистка текста...\")\n",
    "combined_data['text_cleaned'] = combined_data[text_col].apply(clean_text)\n",
    "\n",
    "# Создание бинарных меток\n",
    "combined_data['label_binary'] = combined_data['label'].map({'fake': 1, 'real': 0})\n",
    "\n",
    "# Удаление пустых текстов\n",
    "combined_data = combined_data[\n",
    "    combined_data['text_cleaned'].notna() & \n",
    "    (combined_data['text_cleaned'].str.len() > 0)\n",
    "]\n",
    "\n",
    "print(f\"\\nОбъединенный датасет: {combined_data.shape}\")\n",
    "print(f\"Распределение меток: {combined_data['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# Разделение на train/val/test с стратификацией\n",
    "X = combined_data['text_cleaned'].values\n",
    "y = combined_data['label_binary'].values\n",
    "\n",
    "# Первое разделение: train+val (80%) и test (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Второе разделение: train (64%) и val (16%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"\\nРазделение данных:\")\n",
    "print(f\"  Train: {len(X_train):,} ({len(X_train)/len(combined_data)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(X_val):,} ({len(X_val)/len(combined_data)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(X_test):,} ({len(X_test)/len(combined_data)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Создание словаря и токенизация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_vocab(texts, min_freq=2):\n",
    "    \"\"\"Создание словаря из текстов\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        words = str(text).lower().split()\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    idx = 2\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def text_to_sequence(text, vocab, max_len=256):\n",
    "    \"\"\"Преобразование текста в последовательность индексов\"\"\"\n",
    "    words = str(text).lower().split()\n",
    "    sequence = [vocab.get(word, vocab['<UNK>']) for word in words[:max_len]]\n",
    "    \n",
    "    if len(sequence) < max_len:\n",
    "        sequence.extend([vocab['<PAD>']] * (max_len - len(sequence)))\n",
    "    \n",
    "    return sequence[:max_len]\n",
    "\n",
    "# Создание словаря\n",
    "print(\"\\nСоздание словаря...\")\n",
    "vocab = build_vocab(X_train, min_freq=2)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Размер словаря: {vocab_size}\")\n",
    "\n",
    "MAX_LEN = 256\n",
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Загрузка GloVe embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def load_glove_embeddings(glove_path, vocab, embedding_dim=100):\n",
    "    \"\"\"Загрузка предобученных GloVe embeddings\"\"\"\n",
    "    print(f\"Загрузка GloVe embeddings из {glove_path}...\")\n",
    "    \n",
    "    if not os.path.exists(glove_path):\n",
    "        print(\"Скачивание GloVe 6B.100d...\")\n",
    "        !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "        !unzip -q glove.6B.zip\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    found = 0\n",
    "    \n",
    "    for word, idx in vocab.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[idx] = embeddings_index[word]\n",
    "            found += 1\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    \n",
    "    print(f\"Найдено embeddings для {found}/{vocab_size} слов ({found/vocab_size*100:.2f}%)\")\n",
    "    return embedding_matrix\n",
    "\n",
    "GLOVE_PATH = 'glove.6B.100d.txt'\n",
    "try:\n",
    "    embedding_matrix = load_glove_embeddings(GLOVE_PATH, vocab, EMBEDDING_DIM)\n",
    "    use_pretrained = True\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Не удалось загрузить GloVe: {e}\")\n",
    "    print(\"Используем случайную инициализацию\")\n",
    "    embedding_matrix = None\n",
    "    use_pretrained = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyTorch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        sequence = text_to_sequence(text, self.vocab, self.max_len)\n",
    "        return torch.LongTensor(sequence), torch.LongTensor([label])\n",
    "\n",
    "train_dataset = NewsDataset(X_train, y_train, vocab, MAX_LEN)\n",
    "val_dataset = NewsDataset(X_val, y_val, vocab, MAX_LEN)\n",
    "test_dataset = NewsDataset(X_test, y_test, vocab, MAX_LEN)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters=100, \n",
    "                 filter_sizes=[3, 4, 5], num_classes=2, dropout=0.3, \n",
    "                 embedding_matrix=None):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        # Convolutional layers с разными размерами фильтров\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Conv1d ожидает (batch_size, embedding_dim, seq_len)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_len)\n",
    "        \n",
    "        # Применяем свертки и max pooling\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = conv(embedded)  # (batch_size, num_filters, seq_len - filter_size + 1)\n",
    "            conv_out = torch.relu(conv_out)\n",
    "            pooled = torch.max_pool1d(conv_out, kernel_size=conv_out.size(2))  # (batch_size, num_filters, 1)\n",
    "            pooled = pooled.squeeze(2)  # (batch_size, num_filters)\n",
    "            conv_outputs.append(pooled)\n",
    "        \n",
    "        # Объединяем выходы всех сверток\n",
    "        concatenated = torch.cat(conv_outputs, dim=1)  # (batch_size, len(filter_sizes) * num_filters)\n",
    "        \n",
    "        concatenated = self.dropout(concatenated)\n",
    "        output = self.fc(concatenated)  # (batch_size, num_classes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "cnn_model = CNNModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_filters=100,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    num_classes=2,\n",
    "    dropout=0.3,\n",
    "    embedding_matrix=embedding_matrix if use_pretrained else None\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nCNN Model Parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Функции обучения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.squeeze().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), 100 * correct / total\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Обучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ОБУЧЕНИЕ CNN МОДЕЛИ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "best_f1 = 0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(cnn_model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(cnn_model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(cnn_model.state_dict(), 'best_cnn_model.pth')\n",
    "        print(f\"✓ New best F1: {best_f1:.4f}, model saved\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Лучший F1 на валидации: {best_f1:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Оценка на тестовом наборе\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "\n",
    "print(\"\\nОценка CNN модели на тестовом наборе:\")\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n",
    "    cnn_model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['Real', 'Fake']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.title('CNN - Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "cnn_results = {\n",
    "    'test_loss': float(test_loss),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_f1': float(test_f1),\n",
    "    'test_precision': float(precision_score(test_labels, test_preds, average='weighted')),\n",
    "    'test_recall': float(recall_score(test_labels, test_preds, average='weighted'))\n",
    "}\n",
    "\n",
    "print(f\"\\nCNN Results: {cnn_results}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
