{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\"\"\"\nCNN Model Training Script for Fake News Classification\nЭтот скрипт можно скопировать в Google Colab для обучения CNN модели\n\"\"\"\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# УСТАНОВКА ЗАВИСИМОСТЕЙ (выполнить в Colab)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\"\"\"\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install transformers scikit-learn pandas numpy matplotlib seaborn tqdm\n!pip install gensim\n\"\"\"\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ИМПОРТЫ\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Проверка GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ЗАГРУЗКА ДАННЫХ\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Вариант 1: Загрузка из GitHub\nREPO_URL = \"https://raw.githubusercontent.com/YOUR_USERNAME/Fake-News-Classifier/main\"\n\n# Вариант 2: Загрузка файлов вручную в Colab\n# Используйте: Files -> Upload и загрузите файлы из data/processed/\n\ntry:\n    isot_df = pd.read_csv(f'{REPO_URL}/data/processed/isot_processed.csv')\n    print(f\"✓ ISOT dataset loaded: {isot_df.shape}\")\nexcept:\n    print(\"⚠ Загрузите isot_processed.csv вручную в Colab\")\n    # isot_df = pd.read_csv('/content/isot_processed.csv')\n\ntry:\n    liar_train = pd.read_csv(f'{REPO_URL}/data/processed/liar_train_processed.csv')\n    print(f\"✓ LIAR train loaded: {liar_train.shape}\")\nexcept:\n    print(\"⚠ Загрузите liar_train_processed.csv вручную в Colab\")\n    # liar_train = pd.read_csv('/content/liar_train_processed.csv')\n\n# Объединение данных\nif 'isot_df' in locals() and 'liar_train' in locals():\n    isot_data = isot_df[['text', 'label_binary']].copy()\n    isot_data.columns = ['text', 'label']\n    liar_data = liar_train[['text', 'label_binary']].copy()\n    liar_data.columns = ['text', 'label']\n    combined_data = pd.concat([isot_data, liar_data], ignore_index=True)\nelif 'isot_df' in locals():\n    combined_data = isot_df[['text', 'label_binary']].copy()\n    combined_data.columns = ['text', 'label']\nelif 'liar_train' in locals():\n    combined_data = liar_train[['text', 'label_binary']].copy()\n    combined_data.columns = ['text', 'label']\nelse:\n    raise ValueError(\"Нет данных для обучения!\")\n\n# Очистка\ncombined_data = combined_data[combined_data['text'].notna() & (combined_data['text'].str.len() > 0)]\nprint(f\"\\nОбъединенный датасет: {combined_data.shape}\")\nprint(f\"Распределение меток: {combined_data['label'].value_counts().to_dict()}\")\n\n# Разделение на train/val/test\nX = combined_data['text'].values\ny = combined_data['label'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n)\n\nprint(f\"\\nTrain: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# СОЗДАНИЕ СЛОВАРЯ И ТОКЕНИЗАЦИЯ\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\ndef build_vocab(texts, min_freq\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "2):\n    \"\"\"Создание словаря из текстов\"\"\"\n    word_counts = Counter()\n    for text in texts:\n        words = str(text).lower().split()\n        word_counts.update(words)\n    \n    vocab = {'<PAD>': 0, '<UNK>': 1}\n    idx = 2\n    \n    for word, count in word_counts.items():\n        if count >= min_freq:\n            vocab[word] = idx\n            idx += 1\n    \n    return vocab\n\ndef text_to_sequence(text, vocab, max_len=256):\n    \"\"\"Преобразование текста в последовательность индексов\"\"\"\n    words = str(text).lower().split()\n    sequence = [vocab.get(word, vocab['<UNK>']) for word in words[:max_len]]\n    \n    if len(sequence) < max_len:\n        sequence.extend([vocab['<PAD>']] * (max_len - len(sequence)))\n    \n    return sequence[:max_len]\n\n# Создание словаря\nprint(\"\\nСоздание словаря...\")\nvocab = build_vocab(X_train, min_freq=2)\nvocab_size = len(vocab)\nprint(f\"Размер словаря: {vocab_size}\")\n\nMAX_LEN = 256\nEMBEDDING_DIM = 100\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ЗАГРУЗКА GLOVE EMBEDDINGS\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\ndef load_glove_embeddings(glove_path, vocab, embedding_dim\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "100):\n    \"\"\"Загрузка предобученных GloVe embeddings\"\"\"\n    print(f\"Загрузка GloVe embeddings из {glove_path}...\")\n    \n    if not os.path.exists(glove_path):\n        print(\"Скачивание GloVe 6B.100d...\")\n        os.system('wget http://nlp.stanford.edu/data/glove.6B.zip')\n        os.system('unzip -q glove.6B.zip')\n    \n    embeddings_index = {}\n    with open(glove_path, 'r', encoding='utf-8') as f:\n        for line in tqdm(f, desc=\"Loading GloVe\"):\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    \n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n    found = 0\n    \n    for word, idx in vocab.items():\n        if word in embeddings_index:\n            embedding_matrix[idx] = embeddings_index[word]\n            found += 1\n        else:\n            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n    \n    print(f\"Найдено embeddings для {found}/{vocab_size} слов ({found/vocab_size*100:.2f}%)\")\n    return embedding_matrix\n\nGLOVE_PATH = 'glove.6B.100d.txt'\ntry:\n    embedding_matrix = load_glove_embeddings(GLOVE_PATH, vocab, EMBEDDING_DIM)\n    use_pretrained = True\nexcept Exception as e:\n    print(f\"⚠ Не удалось загрузить GloVe: {e}\")\n    print(\"Используем случайную инициализацию\")\n    embedding_matrix = None\n    use_pretrained = False\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# PYTORCH DATASET\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class NewsDataset(Dataset):\n    def __init__(self, texts, labels, vocab, max_len=256):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        sequence = text_to_sequence(text, self.vocab, self.max_len)\n        return torch.LongTensor(sequence), torch.LongTensor([label])\n\ntrain_dataset = NewsDataset(X_train, y_train, vocab, MAX_LEN)\nval_dataset = NewsDataset(X_val, y_val, vocab, MAX_LEN)\ntest_dataset = NewsDataset(X_test, y_test, vocab, MAX_LEN)\n\nBATCH_SIZE = 16\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# CNN МОДЕЛЬ\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class CNNModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, num_filters=100, \n                 filter_sizes=[3, 4, 5], num_classes=2, dropout=0.3, \n                 embedding_matrix=None):\n        super(CNNModel, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        if embedding_matrix is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n            self.embedding.weight.requires_grad = True\n        \n        # Convolutional layers с разными размерами фильтров\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)\n            for fs in filter_sizes\n        ])\n        \n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n    \n    def forward(self, x):\n        # x shape: (batch_size, seq_len)\n        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n        \n        # Conv1d ожидает (batch_size, embedding_dim, seq_len)\n        embedded = embedded.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_len)\n        \n        # Применяем свертки и max pooling\n        conv_outputs = []\n        for conv in self.convs:\n            conv_out = conv(embedded)  # (batch_size, num_filters, seq_len - filter_size + 1)\n            conv_out = torch.relu(conv_out)\n            pooled = torch.max_pool1d(conv_out, kernel_size=conv_out.size(2))  # (batch_size, num_filters, 1)\n            pooled = pooled.squeeze(2)  # (batch_size, num_filters)\n            conv_outputs.append(pooled)\n        \n        # Объединяем выходы всех сверток\n        concatenated = torch.cat(conv_outputs, dim=1)  # (batch_size, len(filter_sizes) * num_filters)\n        \n        concatenated = self.dropout(concatenated)\n        output = self.fc(concatenated)  # (batch_size, num_classes)\n        \n        return output\n\ncnn_model = CNNModel(\n    vocab_size=vocab_size,\n    embedding_dim=EMBEDDING_DIM,\n    num_filters=100,\n    filter_sizes=[3, 4, 5],\n    num_classes=2,\n    dropout=0.3,\n    embedding_matrix=embedding_matrix if use_pretrained else None\n).to(device)\n\nprint(f\"\\nCNN Model Parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ФУНКЦИИ ОБУЧЕНИЯ\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for sequences, labels in tqdm(train_loader, desc=\"Training\"):\n        sequences = sequences.to(device)\n        labels = labels.squeeze().to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(sequences)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    return total_loss / len(train_loader), 100 * correct / total\n\ndef evaluate(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for sequences, labels in tqdm(val_loader, desc=\"Evaluating\"):\n            sequences = sequences.to(device)\n            labels = labels.squeeze().to(device)\n            \n            outputs = model(sequences)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(val_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1, all_preds, all_labels\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ОБУЧЕНИЕ\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\nprint(\"\\n\" + \"\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\" * 60)\nprint(\"ОБУЧЕНИЕ CNN МОДЕЛИ\")\nprint(\"=\" * 60)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn_model.parameters(), lr=2e-5)\n\nnum_epochs = 10\nbest_f1 = 0\npatience = 3\npatience_counter = 0\n\ntrain_losses = []\nval_losses = []\ntrain_accs = []\nval_accs = []\nval_f1s = []\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    \n    train_loss, train_acc = train_epoch(cnn_model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc, val_f1, _, _ = evaluate(cnn_model, val_loader, criterion, device)\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_accs.append(train_acc)\n    val_accs.append(val_acc)\n    val_f1s.append(val_f1)\n    \n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}\")\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        patience_counter = 0\n        torch.save(cnn_model.state_dict(), 'best_cnn_model.pth')\n        print(f\"✓ New best F1: {best_f1:.4f}, model saved\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping after {epoch+1} epochs\")\n            break\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"Лучший F1 на валидации: {best_f1:.4f}\")\nprint(\"=\" * 60)\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ОЦЕНКА НА ТЕСТОВОМ НАБОРЕ\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ============================================================================\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "cnn_model.load_state_dict(torch.load('best_cnn_model.pth'))\n\nprint(\"\\nОценка CNN модели на тестовом наборе:\")\ntest_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n    cnn_model, test_loader, criterion, device\n)\n\nprint(f\"\\nTest Results:\")\nprint(f\"  Loss: {test_loss:.4f}\")\nprint(f\"  Accuracy: {test_acc:.4f}\")\nprint(f\"  F1-Score: {test_f1:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(test_labels, test_preds, target_names=['Real', 'Fake']))\n\n# Confusion Matrix\ncm = confusion_matrix(test_labels, test_preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\nplt.title('CNN - Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\ncnn_results = {\n    'test_loss': float(test_loss),\n    'test_accuracy': float(test_acc),\n    'test_f1': float(test_f1),\n    'test_precision': float(precision_score(test_labels, test_preds, average='weighted')),\n    'test_recall': float(recall_score(test_labels, test_preds, average='weighted'))\n}\n\nprint(f\"\\nCNN Results: {cnn_results}\")\n\n",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}